{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/anishajauhari/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/anishajauhari/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import warnings\n",
    "import math\n",
    "import nltk\n",
    "import json\n",
    "from langdetect import detect\n",
    "from libsvm import *\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from IPython.display import display\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import svm\n",
    "import dask.dataframe as dd\n",
    "import multiprocessing\n",
    "import swifter\n",
    "warnings.filterwarnings('ignore')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "spell = SpellChecker()\n",
    "\n",
    "# pd.set_option('display.max_colwidth', None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rock_pop_dataset = pd.read_csv(\"../../data/rockpopdataset.csv\")\n",
    "rock_pop_dataset[\"filtered_lyrics\"].fillna('', inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Frequency Inverse Document Frequency Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Unnamed: 0, song, year, artist, genre, lyrics, filtered_lyrics]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "tf_vec = TfidfVectorizer(analyzer = 'word', min_df = 5, max_df = 0.95)\n",
    "X = tf_vec.fit_transform(rock_pop_dataset[\"filtered_lyrics\"]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df = pd.DataFrame(X, columns = tf_vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = np.argsort(np.asarray(X.sum(axis=0)).ravel())[::-1]\n",
    "tfidf_feature_names = np.array(tf_vec.get_feature_names())\n",
    "tf_idf_100 = tfidf_feature_names[importance[:100]]\n",
    "tf_idf_200 = tfidf_feature_names[importance[:200]]\n",
    "tf_idf_500 = tfidf_feature_names[importance[:500]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df_100 = tfidf_df.loc[:, tfidf_df.columns.isin(tf_idf_100)]\n",
    "tfidf_df_200 = tfidf_df.loc[:, tfidf_df.columns.isin(tf_idf_200)]\n",
    "tfidf_df_500 = tfidf_df.loc[:, tfidf_df.columns.isin(tf_idf_500)]\n",
    "\n",
    "tfidf_df_100[\"index\"] = rock_pop_dataset.index\n",
    "tfidf_df_200[\"index\"] = rock_pop_dataset.index\n",
    "tfidf_df_500[\"index\"] = rock_pop_dataset.index\n",
    "\n",
    "# tfidf_df_100[\"misspelled_words\"] = rock_pop_dataset[\"misspelled_words\"]\n",
    "# tfidf_df_200[\"misspelled_words\"] = rock_pop_dataset[\"misspelled_words\"]\n",
    "# tfidf_df_500[\"misspelled_words\"] = rock_pop_dataset[\"misspelled_words\"]\n",
    "\n",
    "# tfidf_df_100[\"unique_words\"] = rock_pop_dataset[\"unique_words\"]\n",
    "# tfidf_df_200[\"unique_words\"] = rock_pop_dataset[\"unique_words\"]\n",
    "# tfidf_df_500[\"unique_words\"] = rock_pop_dataset[\"unique_words\"]\n",
    "# #'danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness','acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo'\n",
    "# tfidf_df_100[\"slang_words\"] = rock_pop_dataset[\"slang_words\"]\n",
    "# tfidf_df_200[\"slang_words\"] = rock_pop_dataset[\"slang_words\"]\n",
    "# tfidf_df_500[\"slang_words\"] = rock_pop_dataset[\"slang_words\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df_100.to_csv(\"../../data/tfidf_100\")\n",
    "tfidf_df_200.to_csv(\"../../data/tfidf_200\")\n",
    "tfidf_df_500.to_csv(\"../../data/tfidf_500\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Gain Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer='word', min_df = 5, max_df = 0.95)\n",
    "X = vectorizer.fit_transform(rock_pop_dataset[\"filtered_lyrics\"]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_terms = pd.DataFrame(vectorizer.get_feature_names(), columns=[\"Term\"])\n",
    "X = pd.DataFrame(X, columns = vectorizer.get_feature_names())\n",
    "# X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rock = rock_pop_dataset.loc[rock_pop_dataset[\"genre\"] == \"Rock\"][\"filtered_lyrics\"]\n",
    "pop = rock_pop_dataset.loc[rock_pop_dataset[\"genre\"] == \"Pop\"][\"filtered_lyrics\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_terms[\"rock\"] = feature_terms[\"Term\"].swifter.apply(lambda a : sum(rock.str.contains(a)))\n",
    "# feature_terms[\"pop\"] = feature_terms[\"Term\"].swifter.apply(lambda a : sum(pop.str.contains(a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_terms[\"rock\"] = dd.from_pandas(feature_terms[\"Term\"], npartitions=4*multiprocessing.cpu_count()).map_partitions(lambda dframe: dframe.apply(lambda a: sum(rock.str.contains(a)))).compute(scheduler='processes')\n",
    "feature_terms[\"pop\"] = dd.from_pandas(feature_terms[\"Term\"], npartitions=4*multiprocessing.cpu_count()).map_partitions(lambda dframe: dframe.apply(lambda a: sum(pop.str.contains(a)))).compute(scheduler='processes')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_terms[\"rock\"] = feature_terms[\"Term\"].apply(lambda a : sum(rock.str.contains(a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_terms[\"pop\"] = feature_terms[\"Term\"].apply(lambda a : sum(pop.str.contains(a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_terms[\"not_rock\"] = 94386 - feature_terms[\"rock\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_terms[\"not_pop\"] = 31157 - feature_terms[\"pop\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def informationGain(term, entropy):\n",
    "    prob_rock = term[\"rock\"]/(term[\"rock\"] + term[\"pop\"])\n",
    "    prob_pop = term[\"pop\"]/(term[\"rock\"] + term[\"pop\"])\n",
    "    prob_not_rock = term[\"not_rock\"]/(term[\"not_rock\"] + term[\"not_pop\"])\n",
    "    prob_not_pop = term[\"not_pop\"]/(term[\"not_rock\"] + term[\"not_pop\"])\n",
    "    total_prob = (term[\"rock\"] + term[\"pop\"])/125543\n",
    "    not_total_prob = (term[\"not_rock\"] + term[\"not_pop\"])/125543\n",
    "    \n",
    "    final_entropy = (total_prob*(-1*((prob_rock*math.log2(prob_rock) if prob_rock!=0 else 0) + (prob_pop*math.log2(prob_pop)if prob_pop!=0 else 0)))) + (not_total_prob*(-1*((prob_not_rock*math.log2(prob_not_rock) if prob_not_rock!=0 else 0) + (prob_not_pop*math.log2(prob_not_pop)if prob_not_pop!=0 else 0))))\n",
    "        \n",
    "    return entropy - final_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy= -1 * (94386/125543) * math.log2(94386/125543) - (31157/125543) * math.log2(31157/125543)\n",
    "feature_terms[\"info_gain\"] = feature_terms.apply(lambda x : informationGain(x, entropy), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_terms = feature_terms.sort_values(by=[\"info_gain\"], ascending=False)\n",
    "# feature_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_terms[:100][\"Term\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ig_terms_100 = feature_terms[:100][\"Term\"]\n",
    "ig_terms_200 = feature_terms[:200][\"Term\"]\n",
    "ig_terms_500 = feature_terms[:500][\"Term\"]\n",
    "\n",
    "ig_100 = X.loc[:, X.columns.isin(ig_terms_100)]\n",
    "ig_200 = X.loc[:, X.columns.isin(ig_terms_200)]\n",
    "ig_500 = X.loc[:, X.columns.isin(ig_terms_500)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ig_500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ig_100[\"index\"] = rock_pop_dataset.index\n",
    "ig_200[\"index\"] = rock_pop_dataset.index\n",
    "ig_500[\"index\"] = rock_pop_dataset.index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ig_100.to_csv(\"../../data/ig_100\")\n",
    "ig_200.to_csv(\"../../data/ig_200\")\n",
    "ig_500.to_csv(\"../../data/ig_500\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
